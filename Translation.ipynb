{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amr/anaconda3/envs/torch/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to /home/amr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from stopwordsallforms import STOPWORDS as arb_stopwords\n",
    "import elements\n",
    "#nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "### Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(t):\n",
    "    return t.translate(t.maketrans(''.join(elements.ALEF_HAMZA_FORMS+elements.NON_ALIF_HAMZA_FORMS+tuple(elements.ALIF_MAQSURA)),\"اااويي\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_stopwords = [clean_str(normalization(i)) for  i in set(arb_stopwords.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(t,stop=arb_stopwords):\n",
    "    return ' '.join([word for word in word_tokenize(t) if word not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.load('../translation project/AD_NMT-master/LAV-MSA-2-both.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش',\n",
       " 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] # lav , msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate two types of text\n",
    "lav=[]\n",
    "msa=[]\n",
    "for i,ex in enumerate(data):\n",
    "    lav_text = stopwords_removal(clean_str(normalization(ex[0])))\n",
    "    msa_text = stopwords_removal(clean_str(normalization(ex[1])))\n",
    "    data[i][0],data[i][1] = lav_text,msa_text\n",
    "    lav.append(lav_text)\n",
    "    msa.append(msa_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lav = ' '.join(lav)\n",
    "msa = ' '.join(msa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_d=collections.Counter(msa.split())\n",
    "lav_d=collections.Counter(lav.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2msa = np.array([word for word,freq in msa_d.items() if freq > min_count ])\n",
    "idx2lav = np.array([word for word,freq in lav_d.items() if freq > min_count ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa2idx = {word:i for i,word in enumerate(idx2msa)}\n",
    "lav2idx = {word:i for i,word in enumerate(idx2lav)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_data = [' '.join([i for i in t[1].split() if (msa2idx.get(i,-1) != -1)]) for t in data]\n",
    "lav_data = [' '.join([i for i in t[0].split() if (lav2idx.get(i,-1) != -1)]) for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 50 random integer\n",
    "np.random.seed(42)\n",
    "random_index = np.random.randint(0,min(len(lav2idx),len(msa2idx)), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_msa = idx2msa[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = nltk.ISRIStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_start = time.time()\n",
    "sim_lav = []\n",
    "sim_msa = []\n",
    "sim=[]\n",
    "alone_msa = []\n",
    "for l,m in zip(lav_data,msa_data): #loop over data\n",
    "    msa_words,lav_words=m.split(),l.split() \n",
    "    stems=[st.stem(i) for i in lav_words]\n",
    "    \n",
    "    for c,j in enumerate(msa_words):\n",
    "        msa_stem = st.stem(j)\n",
    "        if msa_stem in stems:\n",
    "            lav_idx = stems.index(msa_stem)\n",
    "            sim.append((j,lav_words[lav_idx])) #append pair\n",
    "            sim_lav.append(lav_words[lav_idx]) \n",
    "            sim_msa.append(j)\n",
    "stem_end = time.time()\n",
    "stem_time = stem_end-stem_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa2lav_stem = {i:v for i,v in sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_res = []\n",
    "for i in idx2msa[random_index]:\n",
    "    sim_res.append((i,msa2lav_stem.get(i,'None')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_lav = [i[1] for i in sim_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach: Coocurence Matrix\n",
    "In order to get word pairs from lav to msa we try coocurence matric approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc = np.zeros((len(idx2msa), len(idx2lav) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6118, 5272)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_start= time.time()\n",
    "for l,m in zip(lav_data,msa_data):\n",
    "    l,m = l.split(),m.split()\n",
    "    lav_count = collections.Counter(l)\n",
    "    msa_count = collections.Counter(m)\n",
    "    for k_lav,v_lav in lav_count.items():    \n",
    "        for k_msa,v_msa in msa_count.items():\n",
    "            cooc[ msa2idx[k_msa] , lav2idx[k_lav] ] += (min(v_msa,v_lav)/max(v_lav,v_msa))\n",
    "cooc_end= time.time()\n",
    "cooc_time = cooc_end-cooc_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_match = np.argmax(cooc[random_index],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cooc_res = []\n",
    "for msa_idx,lav_idx in zip(random_index,cooc_match):\n",
    "    cooc_res.append((idx2msa[msa_idx],idx2lav[lav_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_lav = [i[1] for i in cooc_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third approach : word vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count occurence of each word per text\n",
    "lav_d_list=[]\n",
    "msa_d_list=[]\n",
    "for lav,msa in zip(lav_data,msa_data):\n",
    "    lav_d_list.append(collections.Counter(lav.split()))\n",
    "    msa_d_list.append(collections.Counter(msa.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vector(ds_freq:dict,word:str):\n",
    "    wv=[]\n",
    "    #l: Counter dictionary\n",
    "    for j,l in enumerate(ds_freq):\n",
    "        wv.append(l.get(word,0))\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_word_vectors(dataset:dict,ds_d_list:dict):\n",
    "    dataset_wv = []\n",
    "    for i,w in enumerate(dataset.keys()): #loop over words\n",
    "        wv = create_word_vector(ds_d_list,w)\n",
    "        dataset_wv.append(wv)\n",
    "    return dataset_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_word_vector=create_ds_word_vectors(msa2idx,msa_d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lav_word_vector=create_ds_word_vectors(lav2idx,lav_d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list of lists to matrix \n",
    "m_vec = np.stack(msa_word_vector)\n",
    "l_vec = np.stack(lav_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_cos_sim_cuda(X,Y):\n",
    "    X,Y = torch.from_numpy(X).type(torch.float).cuda(),torch.from_numpy(Y).type(torch.float).cuda()\n",
    "    mod_v1 = torch.linalg.norm(X,axis=-1)\n",
    "    mod_v2 = torch.linalg.norm(Y,axis=-1)\n",
    "    mod = torch.matmul(mod_v1.view(-1,1),mod_v2.view(1,-1))\n",
    "    dot = torch.matmul(X,Y.T)\n",
    "    return (dot/mod).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_cos_sim(X,Y):\n",
    "    mod_v1 = np.linalg.norm(X,axis=-1)\n",
    "    mod_v2 = np.linalg.norm(Y,axis=-1)\n",
    "    mod = np.dot(mod_v1.reshape(-1,1),mod_v2.reshape(1,-1))\n",
    "\n",
    "    dot = np.dot(X,Y.T)\n",
    "    return (dot/mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_start=time.time()\n",
    "sim_matrix = vectorized_cos_sim_cuda(m_vec,l_vec)\n",
    "cos_sim_match = np.argmax(sim_matrix,axis=1)\n",
    "sim_end=time.time()\n",
    "cos_sim_time = sim_end-sim_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_res = []\n",
    "for msa_idx in random_index:\n",
    "    lav_idx = cos_sim_match[msa_idx]\n",
    "    cos_sim_res.append((idx2msa[msa_idx],idx2lav[lav_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_lav = [i[1] for i in cos_sim_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Approach: K-nearest neighbours\n",
    " <ol>\n",
    "  <li>get word emb </li>\n",
    "  <li>document vector and ind2doc dictionary </li>\n",
    "  <li>calculate number of planes and number of universes </li>\n",
    "  <li>implment hash puckets</li>\n",
    "  <li>creat hash tables and universes</li>\n",
    "  <li>approximate knn universes</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the rules of the assignment, let us say that in average we want to have 128 example in each index, therefore we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_per_index = 128\n",
    "num_of_planes = np.ceil(np.log2(len(data)/examples_per_index)).astype(int)\n",
    "num_of_planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_universes=20\n",
    "n_dim = l_vec.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash puckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = np.random.normal(0,1,(num_of_universes,n_dim,num_of_planes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_hash(mat,planes):\n",
    "    num_of_universes,_,num_planes = planes.shape\n",
    "    num_puckets=2**num_planes\n",
    "\n",
    "    temp = np.dot(mat,planes) #num_words x num_of_universes x num_planes\n",
    "    temp = np.sign(temp) == 1\n",
    "    \n",
    "    pow_2 = np.power(2,np.arange(num_planes))\n",
    "    \n",
    "    #Get the place where the word will be stored at\n",
    "    hash_index = np.dot(temp,pow_2) #num_words x num_of_universes\n",
    "    \n",
    "    #Create dictionaries to store data in\n",
    "    hash_table = []\n",
    "    id_table = []    \n",
    "    for i in enumerate(range(num_of_universes)):\n",
    "        hash_table.append({i:[] for i in range(num_puckets)})\n",
    "        id_table.append({i:[] for i in range(num_puckets)})\n",
    "    #Fill dictionaries\n",
    "    for universe,hashes in enumerate(hash_index.T):\n",
    "        for i,h in enumerate(hashes):\n",
    "            hash_table[universe][h].append(mat[i])\n",
    "            id_table[universe][h].append(i)\n",
    "        \n",
    "    return hash_table ,id_table ,hash_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_index has the size of given list, contains the index of each word\n",
    "lav_hash2vec,lav_hash2id,_ = matrix_hash(l_vec,planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_index has the size of given list, contains the index of each word\n",
    "_,_,msa_id2hash = matrix_hash(m_vec,planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are words that existed multiple times for the same word.\n",
    "<br>so, we will ignore the words that point to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply k-nearsest negihbours\n",
    " <ol>\n",
    "  <li>pass over each word and get its ID index</li>\n",
    "  <li>get the contents from l_id_table</li>\n",
    "  <li>Stack the vectors existed in l_id_table from l_hash_table</li>\n",
    "  <li>Perform cosine similarity </li>\n",
    "  <li>Get the largest value index</li>\n",
    "  <li>define its id</li>\n",
    "  <li>store the word in a list</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_per_word(X,key_id2hash,value_hash2vec,value_hash2id):\n",
    "    lav_idx_per_word=[]\n",
    "    #candidate similarities in each universe\n",
    "    for UNIVERSE,pucket_id in enumerate(key_id2hash): \n",
    "        #extract lav hash table of universe number k\n",
    "        y_vec = value_hash2vec[UNIVERSE][pucket_id]\n",
    "        if len(y_vec) ==0:\n",
    "            continue\n",
    "        Y = np.stack(y_vec)\n",
    "        #calculate similarity between msa vector and k neghibours in lav id \n",
    "        sim = vectorized_cos_sim(X,Y)\n",
    "        max_idx = np.argmax(sim)\n",
    "        #Get its index from id_table\n",
    "        l_word_idx = value_hash2id[UNIVERSE][pucket_id][max_idx]\n",
    "        lav_idx_per_word.append(l_word_idx)\n",
    "    return most_frequent(lav_idx_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_per_list(k_vec,random_index,key_id2hash,value_hash2vec,value_hash2id):\n",
    "    pair = []\n",
    "    #size numWords x n_dim\n",
    "    key_hash_list = key_id2hash[random_index]\n",
    "    \n",
    "    for i,value_ids in enumerate(key_hash_list):\n",
    "        X = k_vec[random_index[i]]\n",
    "        \n",
    "        k_neghibours_match = search_per_word(X,value_ids,value_hash2vec=value_hash2vec,value_hash2id=value_hash2id)\n",
    "        pair.append((random_index[i],k_neghibours_match))\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_n_start = time.time()\n",
    "kn_res = search_per_list(m_vec,random_index,msa_id2hash,lav_hash2vec,lav_hash2id)\n",
    "k_n_end = time.time()\n",
    "k_n_time = k_n_end - k_n_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_n_lav = [idx2lav[i[1]] for i in kn_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Approach: Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    def __init__(self,ds):\n",
    "        self.ds = ds\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __iter__(self):\n",
    "        for line in self.ds:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lav_corp = MyCorpus(lav_data)\n",
    "msa_corp = MyCorpus(msa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lav = gensim.models.Word2Vec(epochs=10,sentences=lav_corp,min_count=0,vector_size=300)\n",
    "m_msa = gensim.models.Word2Vec(epochs=10,sentences=msa_corp,min_count=0,vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('خير', 0.9969905614852905),\n",
       "  ('السلامه', 0.9969214200973511),\n",
       "  ('خيرا', 0.9965105056762695),\n",
       "  ('نلتقي', 0.9962499737739563),\n",
       "  ('البكالوريا', 0.9956582188606262),\n",
       "  ('نكمل', 0.9956363439559937),\n",
       "  ('قريب', 0.9951711297035217),\n",
       "  ('القي', 0.9949621558189392),\n",
       "  ('واتخلص', 0.9947621822357178),\n",
       "  ('هدي', 0.9943655729293823)],\n",
       " [('خير', 0.9967350959777832),\n",
       "  ('تصبحوا', 0.9945842027664185),\n",
       "  ('قدر', 0.9927830100059509),\n",
       "  ('قريب', 0.9921440482139587),\n",
       "  ('تصبحي', 0.9918696880340576),\n",
       "  ('امان', 0.9917701482772827),\n",
       "  ('ايامك', 0.9914921522140503),\n",
       "  ('انشاء', 0.9914281368255615),\n",
       "  ('يسامحك', 0.9909295439720154),\n",
       "  ('السلامه', 0.9907274842262268)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_msa.wv.most_similar('الله'),m_lav.wv.most_similar('الله')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_start = time.time()\n",
    "sim = vectorized_cos_sim(m_msa.wv.vectors,m_lav.wv.vectors)\n",
    "lm_end = time.time()\n",
    "lm_time = lm_end - lm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_match = np.argmax(sim,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_res = []\n",
    "for msa_idx in random_index:\n",
    "    lav_idx = lm_match[msa_idx]\n",
    "    lm_res.append((idx2msa[msa_idx],idx2lav[lav_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_lav = [i[1] for i in lm_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_msa = list(src_msa)\n",
    "src_msa.append('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_lav.append(stem_time)\n",
    "cooc_lav.append(cooc_time)\n",
    "cos_sim_lav.append(cos_sim_time)\n",
    "k_n_lav.append(k_n_time)\n",
    "lm_lav.append(lm_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['msa'] = src_msa\n",
    "df['stem_sim'] = stem_lav\n",
    "df['cooc'] = cooc_lav\n",
    "df['cos_sim'] = cos_sim_lav\n",
    "df['k-neighbours'] = k_n_lav\n",
    "df['word2vec'] = lm_lav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem_sim</th>\n",
       "      <th>cooc</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>k-neighbours</th>\n",
       "      <th>word2vec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msa</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>بهاء</th>\n",
       "      <td>بهاء</td>\n",
       "      <td>بهاء</td>\n",
       "      <td>بهاء</td>\n",
       "      <td>بهاء</td>\n",
       "      <td>اتركيه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>اثق</th>\n",
       "      <td>None</td>\n",
       "      <td>ممكن</td>\n",
       "      <td>ممكن</td>\n",
       "      <td>الجزاير</td>\n",
       "      <td>بتعيش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>احسنت</th>\n",
       "      <td>احسنت</td>\n",
       "      <td>احسنت</td>\n",
       "      <td>احسنت</td>\n",
       "      <td>احسنت</td>\n",
       "      <td>للرجال</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>مامعني</th>\n",
       "      <td>None</td>\n",
       "      <td>كلمه</td>\n",
       "      <td>سلسبيل</td>\n",
       "      <td>سلسبيل</td>\n",
       "      <td>بتقع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>نمزح</th>\n",
       "      <td>None</td>\n",
       "      <td>عم</td>\n",
       "      <td>احنا</td>\n",
       "      <td>عشره</td>\n",
       "      <td>تشتغلي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>انهض</th>\n",
       "      <td>None</td>\n",
       "      <td>قوم</td>\n",
       "      <td>قوم</td>\n",
       "      <td>قوم</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>نوره</th>\n",
       "      <td>نوره</td>\n",
       "      <td>نوره</td>\n",
       "      <td>نوره</td>\n",
       "      <td>نوره</td>\n",
       "      <td>استوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>جمعت</th>\n",
       "      <td>جمعت</td>\n",
       "      <td>جمعت</td>\n",
       "      <td>جمعت</td>\n",
       "      <td>جمعت</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>خايف</th>\n",
       "      <td>خايف</td>\n",
       "      <td>خايف</td>\n",
       "      <td>القرد</td>\n",
       "      <td>خايفه</td>\n",
       "      <td>نزلت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>طبيعه</th>\n",
       "      <td>طبيعه</td>\n",
       "      <td>طبيعه</td>\n",
       "      <td>طبيعه</td>\n",
       "      <td>طبيعه</td>\n",
       "      <td>الحقد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>بالطبع</th>\n",
       "      <td>طبعا</td>\n",
       "      <td>طبعا</td>\n",
       "      <td>طبعا</td>\n",
       "      <td>11</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سيشعر</th>\n",
       "      <td>None</td>\n",
       "      <td>زي</td>\n",
       "      <td>يقرا</td>\n",
       "      <td>قصدي</td>\n",
       "      <td>المركز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>العشاء</th>\n",
       "      <td>العشاء</td>\n",
       "      <td>العشاء</td>\n",
       "      <td>العشاء</td>\n",
       "      <td>بعدين</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>متزوجا</th>\n",
       "      <td>متزوجه</td>\n",
       "      <td>متزوج</td>\n",
       "      <td>متزوج</td>\n",
       "      <td>متزوج</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تكتب</th>\n",
       "      <td>اكتب</td>\n",
       "      <td>تكتب</td>\n",
       "      <td>تكتب</td>\n",
       "      <td>تكتب</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>بطاقتي</th>\n",
       "      <td>None</td>\n",
       "      <td>الكليه</td>\n",
       "      <td>الكليه</td>\n",
       "      <td>الشخصيه</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>مغلقا</th>\n",
       "      <td>None</td>\n",
       "      <td>مسكر</td>\n",
       "      <td>مسكر</td>\n",
       "      <td>تليفونك</td>\n",
       "      <td>استاذه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يحسن</th>\n",
       "      <td>يحسن</td>\n",
       "      <td>مستواه</td>\n",
       "      <td>يحسن</td>\n",
       "      <td>يحسن</td>\n",
       "      <td>معلق</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>امك</th>\n",
       "      <td>امك</td>\n",
       "      <td>امك</td>\n",
       "      <td>امك</td>\n",
       "      <td>امك</td>\n",
       "      <td>اميره</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ساترككم</th>\n",
       "      <td>None</td>\n",
       "      <td>السلامه</td>\n",
       "      <td>اترككم</td>\n",
       "      <td>الليل</td>\n",
       "      <td>المركز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تنبض</th>\n",
       "      <td>None</td>\n",
       "      <td>حياه</td>\n",
       "      <td>حبيب</td>\n",
       "      <td>حبيب</td>\n",
       "      <td>ودكاتره</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>والطين</th>\n",
       "      <td>None</td>\n",
       "      <td>الخشب</td>\n",
       "      <td>الخشب</td>\n",
       "      <td>الخشب</td>\n",
       "      <td>الحمص</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>اقول</th>\n",
       "      <td>اقولك</td>\n",
       "      <td>اقولك</td>\n",
       "      <td>اقولك</td>\n",
       "      <td>مرحبا</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>فريد</th>\n",
       "      <td>فريد</td>\n",
       "      <td>فريد</td>\n",
       "      <td>فريد</td>\n",
       "      <td>فريد</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>انتظارك</th>\n",
       "      <td>None</td>\n",
       "      <td>تحياتي</td>\n",
       "      <td>تحياتي</td>\n",
       "      <td>عدت</td>\n",
       "      <td>جربت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>العمليه</th>\n",
       "      <td>عملت</td>\n",
       "      <td>عملت</td>\n",
       "      <td>العمليه</td>\n",
       "      <td>العمليه</td>\n",
       "      <td>اسمك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الهرب</th>\n",
       "      <td>None</td>\n",
       "      <td>ليش</td>\n",
       "      <td>وربع</td>\n",
       "      <td>يحاولوا</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>اعدت</th>\n",
       "      <td>None</td>\n",
       "      <td>جديد</td>\n",
       "      <td>عدت</td>\n",
       "      <td>عدت</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ضيقي</th>\n",
       "      <td>None</td>\n",
       "      <td>اكون</td>\n",
       "      <td>اكون</td>\n",
       "      <td>اكون</td>\n",
       "      <td>لحالي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>رياضيات</th>\n",
       "      <td>رياضه</td>\n",
       "      <td>رياضيات</td>\n",
       "      <td>رياضيات</td>\n",
       "      <td>رياضيات</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>اعرفه</th>\n",
       "      <td>بعرف</td>\n",
       "      <td>بعرفه</td>\n",
       "      <td>بعرفو</td>\n",
       "      <td>بعرفش</td>\n",
       "      <td>حسناتك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>بالمال</th>\n",
       "      <td>None</td>\n",
       "      <td>بمصاري</td>\n",
       "      <td>بمصاري</td>\n",
       "      <td>بمصاري</td>\n",
       "      <td>المركز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>رجاءا</th>\n",
       "      <td>None</td>\n",
       "      <td>حدا</td>\n",
       "      <td>الحمام</td>\n",
       "      <td>الحمام</td>\n",
       "      <td>بلهجه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يحضره</th>\n",
       "      <td>None</td>\n",
       "      <td>ياه</td>\n",
       "      <td>يجيبلك</td>\n",
       "      <td>علبه</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الفتيات</th>\n",
       "      <td>None</td>\n",
       "      <td>البنات</td>\n",
       "      <td>البنات</td>\n",
       "      <td>البنات</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الضجيج</th>\n",
       "      <td>None</td>\n",
       "      <td>بيحكي</td>\n",
       "      <td>دوشه</td>\n",
       "      <td>اوعي</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تسليه</th>\n",
       "      <td>تسليه</td>\n",
       "      <td>تسليه</td>\n",
       "      <td>تسليه</td>\n",
       "      <td>تسليه</td>\n",
       "      <td>احاول</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>وغزه</th>\n",
       "      <td>وغزه</td>\n",
       "      <td>دم</td>\n",
       "      <td>بيروت</td>\n",
       "      <td>بيروت</td>\n",
       "      <td>لموضوع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يده</th>\n",
       "      <td>None</td>\n",
       "      <td>حب</td>\n",
       "      <td>وغرام</td>\n",
       "      <td>اعوذ</td>\n",
       "      <td>العريس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الازل</th>\n",
       "      <td>None</td>\n",
       "      <td>زمان</td>\n",
       "      <td>علاج</td>\n",
       "      <td>كويس</td>\n",
       "      <td>ميزان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تكلم</th>\n",
       "      <td>الكلام</td>\n",
       "      <td>هيك</td>\n",
       "      <td>الاختصاص</td>\n",
       "      <td>الاختصاص</td>\n",
       "      <td>تزيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الاساس</th>\n",
       "      <td>الاساس</td>\n",
       "      <td>موضوع</td>\n",
       "      <td>والحفلات</td>\n",
       "      <td>عدت</td>\n",
       "      <td>صباحك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ديسمبر</th>\n",
       "      <td>None</td>\n",
       "      <td>الاول</td>\n",
       "      <td>كانون</td>\n",
       "      <td>كانون</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ستفعل</th>\n",
       "      <td>None</td>\n",
       "      <td>شو</td>\n",
       "      <td>واشوف</td>\n",
       "      <td>الليله</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الجريده</th>\n",
       "      <td>الجريده</td>\n",
       "      <td>بالجريده</td>\n",
       "      <td>الجريده</td>\n",
       "      <td>بينفع</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>انترسبيتش</th>\n",
       "      <td>انترسبيتش</td>\n",
       "      <td>انترسبيتش</td>\n",
       "      <td>انترسبيتش</td>\n",
       "      <td>انترسبيتش</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>مجامله</th>\n",
       "      <td>مجامله</td>\n",
       "      <td>والله</td>\n",
       "      <td>مجامله</td>\n",
       "      <td>مجامله</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>سباق</th>\n",
       "      <td>None</td>\n",
       "      <td>دايما</td>\n",
       "      <td>يسعد</td>\n",
       "      <td>يسعد</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تنتبه</th>\n",
       "      <td>انتبهت</td>\n",
       "      <td>اللي</td>\n",
       "      <td>انتبهتش</td>\n",
       "      <td>بدت</td>\n",
       "      <td>المركز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الغرفه</th>\n",
       "      <td>بالغرفه</td>\n",
       "      <td>الاوضه</td>\n",
       "      <td>الاوضه</td>\n",
       "      <td>الاوضه</td>\n",
       "      <td>بتدافع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.460136</td>\n",
       "      <td>2.7079</td>\n",
       "      <td>3.75657</td>\n",
       "      <td>0.193292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stem_sim       cooc    cos_sim k-neighbours  word2vec\n",
       "msa                                                              \n",
       "بهاء            بهاء       بهاء       بهاء         بهاء    اتركيه\n",
       "اثق             None       ممكن       ممكن      الجزاير     بتعيش\n",
       "احسنت          احسنت      احسنت      احسنت        احسنت    للرجال\n",
       "مامعني          None       كلمه     سلسبيل       سلسبيل      بتقع\n",
       "نمزح            None         عم       احنا         عشره    تشتغلي\n",
       "انهض            None        قوم        قوم          قوم    بتدافع\n",
       "نوره            نوره       نوره       نوره         نوره     استوي\n",
       "جمعت            جمعت       جمعت       جمعت         جمعت         @\n",
       "خايف            خايف       خايف      القرد        خايفه      نزلت\n",
       "طبيعه          طبيعه      طبيعه      طبيعه        طبيعه     الحقد\n",
       "بالطبع          طبعا       طبعا       طبعا           11    بتدافع\n",
       "سيشعر           None         زي       يقرا         قصدي    المركز\n",
       "العشاء        العشاء     العشاء     العشاء        بعدين    بتدافع\n",
       "متزوجا        متزوجه      متزوج      متزوج        متزوج    بتدافع\n",
       "تكتب            اكتب       تكتب       تكتب         تكتب    بتدافع\n",
       "بطاقتي          None     الكليه     الكليه      الشخصيه    بتدافع\n",
       "مغلقا           None       مسكر       مسكر      تليفونك    استاذه\n",
       "يحسن            يحسن     مستواه       يحسن         يحسن      معلق\n",
       "امك              امك        امك        امك          امك     اميره\n",
       "ساترككم         None    السلامه     اترككم        الليل    المركز\n",
       "تنبض            None       حياه       حبيب         حبيب   ودكاتره\n",
       "والطين          None      الخشب      الخشب        الخشب     الحمص\n",
       "اقول           اقولك      اقولك      اقولك        مرحبا    بتدافع\n",
       "فريد            فريد       فريد       فريد         فريد    بتدافع\n",
       "انتظارك         None     تحياتي     تحياتي          عدت      جربت\n",
       "العمليه         عملت       عملت    العمليه      العمليه      اسمك\n",
       "الهرب           None        ليش       وربع      يحاولوا    بتدافع\n",
       "اعدت            None       جديد        عدت          عدت    بتدافع\n",
       "ضيقي            None       اكون       اكون         اكون     لحالي\n",
       "رياضيات        رياضه    رياضيات    رياضيات      رياضيات    بتدافع\n",
       "اعرفه           بعرف      بعرفه      بعرفو        بعرفش    حسناتك\n",
       "بالمال          None     بمصاري     بمصاري       بمصاري    المركز\n",
       "رجاءا           None        حدا     الحمام       الحمام     بلهجه\n",
       "يحضره           None        ياه     يجيبلك         علبه    بتدافع\n",
       "الفتيات         None     البنات     البنات       البنات    بتدافع\n",
       "الضجيج          None      بيحكي       دوشه         اوعي    بتدافع\n",
       "تسليه          تسليه      تسليه      تسليه        تسليه     احاول\n",
       "وغزه            وغزه         دم      بيروت        بيروت    لموضوع\n",
       "يده             None         حب      وغرام         اعوذ    العريس\n",
       "الازل           None       زمان       علاج         كويس     ميزان\n",
       "تكلم          الكلام        هيك   الاختصاص     الاختصاص      تزيد\n",
       "الاساس        الاساس      موضوع   والحفلات          عدت     صباحك\n",
       "ديسمبر          None      الاول      كانون        كانون    بتدافع\n",
       "ستفعل           None         شو      واشوف       الليله    بتدافع\n",
       "الجريده      الجريده   بالجريده    الجريده        بينفع    بتدافع\n",
       "انترسبيتش  انترسبيتش  انترسبيتش  انترسبيتش    انترسبيتش    بتدافع\n",
       "مجامله        مجامله      والله     مجامله       مجامله    بتدافع\n",
       "سباق            None      دايما       يسعد         يسعد    بتدافع\n",
       "تنتبه         انتبهت       اللي    انتبهتش          بدت    المركز\n",
       "الغرفه       بالغرفه     الاوضه     الاوضه       الاوضه    بتدافع\n",
       "time        0.584471   0.460136     2.7079      3.75657  0.193292"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(index=df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['durations'] = [stem_time,cooc_time,cos_sim_time,k_n_time,lm_time]\n",
    "df2['accuracy'] = np.array([24,25,30,24,0]) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>durations</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stem_sim</th>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooc</th>\n",
       "      <td>0.460136</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_sim</th>\n",
       "      <td>2.707899</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k-neighbours</th>\n",
       "      <td>3.756567</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              durations  accuracy\n",
       "stem_sim       0.584471      0.48\n",
       "cooc           0.460136      0.50\n",
       "cos_sim        2.707899      0.60\n",
       "k-neighbours   3.756567      0.48\n",
       "word2vec       0.193292      0.00"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation\n",
    "## 1.1 Generate embedding and transform matrices\n",
    "We will use the word pair generated by cosine similarity and the embeddings generated by word2vec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [(i,idx2lav[ret[j]]) for i,j in msa2idx.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اعرف', 'بعرف'),\n",
       " ('واحده', 'وحده'),\n",
       " ('ذهبت', 'رحت'),\n",
       " ('فرنسا', 'فرنسا'),\n",
       " ('غرفه', 'اوضه'),\n",
       " ('وضعت', 'حطيت'),\n",
       " ('اذهب', 'روح'),\n",
       " ('تقدم', 'ماشيه'),\n",
       " ('يسارا', 'عاليسار'),\n",
       " ('يجب', 'لازم'),\n",
       " ('يكون', 'يكون'),\n",
       " ('موضوع', 'موضوع'),\n",
       " ('تبتعد', 'تبعد'),\n",
       " ('تفقد', 'تبعد'),\n",
       " ('الطريق', 'الطريق'),\n",
       " ('اقصد', 'قصدي'),\n",
       " ('صراحه', 'صراحه'),\n",
       " ('ياامي', 'يما'),\n",
       " ('ايضا', 'كمان'),\n",
       " ('كرهته', 'وصرت'),\n",
       " ('يوم', 'يوم'),\n",
       " ('حضرته', 'حضرته'),\n",
       " ('مثلما', 'بلح'),\n",
       " ('خالتي', 'خالتي'),\n",
       " ('قال', 'قالي'),\n",
       " ('موضوعك', 'موضوعك'),\n",
       " ('جيد', 'منيح'),\n",
       " ('ساقول', 'حاقولك'),\n",
       " ('كنت', 'كنت'),\n",
       " ('اردت', 'بدي'),\n",
       " ('اخوتي', 'طيبه'),\n",
       " ('اشتري', 'اشتري'),\n",
       " ('الطيبه', 'صدقه'),\n",
       " ('اخر', 'تاني'),\n",
       " ('الليل', 'الليل'),\n",
       " ('متاكده', 'متاكده'),\n",
       " ('تصبح', 'شاعر'),\n",
       " ('عندما', 'تكره'),\n",
       " ('بدوا', 'وصاروا'),\n",
       " ('المنزل', 'البيت'),\n",
       " ('مرضت', 'مرضت'),\n",
       " ('لماذا', 'ليش'),\n",
       " ('كلمات', 'كلمات'),\n",
       " ('جميله', 'حلوه'),\n",
       " ('كليب', 'فيديو'),\n",
       " ('جميل', 'حلو'),\n",
       " ('ويحتوي', 'استغربت'),\n",
       " ('تشجيع', 'استغربت'),\n",
       " ('لبس', 'كيس'),\n",
       " ('الحجاب', 'الحجاب'),\n",
       " ('الحقيقه', 'الحقيقه'),\n",
       " ('تعجبت', 'استغربت'),\n",
       " ('الكلام', 'الكلام'),\n",
       " ('الغريب', 'الغريب'),\n",
       " ('تحدثتي', 'استغربت'),\n",
       " ('الاغنيه', 'الاغنيه'),\n",
       " ('العرض', 'العرض'),\n",
       " ('الامر', 'الموضوع'),\n",
       " ('وظيفه', 'محترمه'),\n",
       " ('محترمه', 'مكياج'),\n",
       " ('تعني', 'دموع'),\n",
       " ('محل', 'محل'),\n",
       " ('للسكن', 'محترمه'),\n",
       " ('وسياره', 'وسياره'),\n",
       " ('يخاف', 'خايف'),\n",
       " ('الحرب', 'الحرب'),\n",
       " ('شي', 'احباط'),\n",
       " ('مهم', 'مهم'),\n",
       " ('يخسره', 'خايف'),\n",
       " ('الجزاير', 'الجزاير'),\n",
       " ('العاصمه', 'العاصمه'),\n",
       " ('الخاصه', 'الخاصه'),\n",
       " ('قليله', 'وبسرعه'),\n",
       " ('المشكله', 'المشكله'),\n",
       " ('اعملي', 'اشتغلي'),\n",
       " ('جيدا', 'منيح'),\n",
       " ('جيت', 'اجيت'),\n",
       " ('تعرف', 'بتعرف'),\n",
       " ('تفعله', 'بتدافع'),\n",
       " ('تحسبين', 'مفكره'),\n",
       " ('العمل', 'الشغل'),\n",
       " ('سهل', 'سهل'),\n",
       " ('بلادنا', 'بلادنا'),\n",
       " ('السلامه', 'السلامه'),\n",
       " ('اكتب', 'اكتب'),\n",
       " ('حاليا', 'حاليا'),\n",
       " ('اسمعها', 'سمعتها'),\n",
       " ('قالت', 'قالتلي'),\n",
       " ('مراد', 'مراد'),\n",
       " ('سامي', 'سامي'),\n",
       " ('يقول', 'بيقول'),\n",
       " ('وجدتك', 'عملته'),\n",
       " ('تحاول', 'تتصل'),\n",
       " ('تفكها', 'عملته'),\n",
       " ('ففعلت', 'عملته'),\n",
       " ('بدلا', 'للتحرش'),\n",
       " ('المهم', 'المهم'),\n",
       " ('عايده', 'عايده'),\n",
       " ('اخذت', 'اخذت'),\n",
       " ('الدوله', 'الدوله'),\n",
       " ('اظن', 'اظني'),\n",
       " ('سليمه', 'سليمه'),\n",
       " ('فعلنا', 'عملنا'),\n",
       " ('بدات', 'بلشت'),\n",
       " ('الحلقه', 'الحلقه'),\n",
       " ('تم', 'سرقه'),\n",
       " ('ترسيمها', 'تثبتت'),\n",
       " ('المسكينه', 'المسكينه'),\n",
       " ('عانت', 'قاست'),\n",
       " ('كثيرا', 'كتير'),\n",
       " ('اداره', 'اداره'),\n",
       " ('مازالت', 'الاجابه'),\n",
       " ('البلد', 'البلد'),\n",
       " ('يتعب', 'بتعب'),\n",
       " ('ويشقي', 'البلد'),\n",
       " ('الشباب', 'الشباب'),\n",
       " ('اخترت', 'اخترت'),\n",
       " ('فيفري', 'شباط'),\n",
       " ('المقايس', 'المقايس'),\n",
       " ('تكمن', 'المشكله'),\n",
       " ('النظام', 'النظام'),\n",
       " ('باكمله', 'خلصي'),\n",
       " ('شيء', 'اشي'),\n",
       " ('اجن', 'بالعصايه'),\n",
       " ('صحيح', 'مزبوط'),\n",
       " ('توجد', 'فيش'),\n",
       " ('حقا', 'عنجد'),\n",
       " ('اكيد', 'اكيد'),\n",
       " ('نوع', 'نوع'),\n",
       " ('التسويق', 'التسويق'),\n",
       " ('بالطبع', 'طبعا'),\n",
       " ('الموضوع', 'الموضوع'),\n",
       " ('المنقول', 'المنقول'),\n",
       " ('يستحق', 'المنقول'),\n",
       " ('بمعني', 'المنقول'),\n",
       " ('يحتوي', 'المنقول'),\n",
       " ('الصفات', 'الصفات'),\n",
       " ('والخصايص', 'المنقول'),\n",
       " ('قيلت', 'المنقول'),\n",
       " ('اخيرا', 'المنقول'),\n",
       " ('شكرا', 'شكرا'),\n",
       " ('اختي', 'اختي'),\n",
       " ('الكريمه', 'الكريمه'),\n",
       " ('الاهتمام', 'المنقول'),\n",
       " ('الشيء', 'الاشي'),\n",
       " ('العالم', 'احمر'),\n",
       " ('اليوم', 'اليوم'),\n",
       " ('سيكون', 'حيكون'),\n",
       " ('احمر', 'احمر'),\n",
       " ('وفوضي', 'احمر'),\n",
       " ('الممكن', 'ممكن'),\n",
       " ('ترجع', 'اجتهاد'),\n",
       " ('كتب', 'اجتهاد'),\n",
       " ('كثيره', 'كتيره'),\n",
       " ('وتقرا', 'اجتهاد'),\n",
       " ('اجتهاد', 'اجتهاد'),\n",
       " ('شيخ', 'شيخ'),\n",
       " ('التفسير', 'اجتهاد'),\n",
       " ('اسالك', 'اسالك'),\n",
       " ('قلت', 'قلتلها'),\n",
       " ('اتصلت', 'اتصلت'),\n",
       " ('مويا', 'مويا'),\n",
       " ('اتمني', 'بتمني'),\n",
       " ('الله', 'الله'),\n",
       " ('مستمع', 'ويارب'),\n",
       " ('ياموني', 'ويارب'),\n",
       " ('مسلسل', 'مسلسل'),\n",
       " ('جديد', 'جديد'),\n",
       " ('يعرض', 'ينعرض'),\n",
       " ('قناه', 'الجزيره'),\n",
       " ('ابوظبي', 'ابوظبي'),\n",
       " ('تكلف', 'بتكلف'),\n",
       " ('رحله', 'رحله'),\n",
       " ('باريس', 'باريس'),\n",
       " ('مثلا', 'مثلا'),\n",
       " ('ياساره', 'ياساره'),\n",
       " ('الامتحان', 'الامتحان'),\n",
       " ('الكتابي', 'الكتابي'),\n",
       " ('قضيت', 'نسيتك'),\n",
       " ('اربع', 'اربع'),\n",
       " ('ساعات', 'ساعات'),\n",
       " ('الحاسوب', 'الكمبيوتر'),\n",
       " ('انفجر', 'انفجر'),\n",
       " ('راسي', 'راسي'),\n",
       " ('درسنا', 'درسنا'),\n",
       " ('ساعتين', 'ساعتين'),\n",
       " ('الاولي', 'الاولي'),\n",
       " ('تاريخ', 'تاريخ'),\n",
       " ('والثانيه', 'انستي'),\n",
       " ('رياضيات', 'رياضيات'),\n",
       " ('الصعب', 'صعب'),\n",
       " ('والعدل', 'العدل'),\n",
       " ('تجده', 'بتلاقيه'),\n",
       " ('والناس', 'والناس'),\n",
       " ('ناس', 'العدل'),\n",
       " ('ماستار', 'المدارس'),\n",
       " ('الخير', 'الخير'),\n",
       " ('الحال', 'الاسبوعيه'),\n",
       " ('طرق', 'طرق'),\n",
       " ('يعملون', 'بيشتغلوا'),\n",
       " ('حسنا', 'طيب'),\n",
       " ('اقترحت', 'اقترحت'),\n",
       " ('رفض', 'رفض'),\n",
       " ('توقفي', 'بطلي'),\n",
       " ('الظلم', 'الظلم'),\n",
       " ('صور', 'صور'),\n",
       " ('اشعه', 'اشعه'),\n",
       " ('دقيقه', 'دقيقه'),\n",
       " ('بالنسبه', 'بالنسبه'),\n",
       " ('الاشعه', 'الاشعه'),\n",
       " ('الدقيقه', 'الدقيقه'),\n",
       " ('المفروض', 'المفروض'),\n",
       " ('موعدا', 'موعد'),\n",
       " ('الاسبوع', 'الاسبوع'),\n",
       " ('السينيه', 'السينيه'),\n",
       " ('منتصف', 'تويتي'),\n",
       " ('النهار', 'النهار'),\n",
       " ('وانتهي', 'وخلص'),\n",
       " ('يذبح', 'يربي'),\n",
       " ('يبقي', 'بيضل'),\n",
       " ('اسبوعا', 'اسبوع'),\n",
       " ('الحج', 'حجت'),\n",
       " ('ساشتري', 'حاشتري'),\n",
       " ('القهوه', 'القهوه'),\n",
       " ('وقتنا', 'وقتنا'),\n",
       " ('رقصت', 'رقصت'),\n",
       " ('معلومات', 'معلومات'),\n",
       " ('جيده', 'منيحه'),\n",
       " ('ورايعه', 'وروعه'),\n",
       " ('لالالالالالا', 'ندرس'),\n",
       " ('نذاكر', 'ندرس'),\n",
       " ('جمعيا', 'ندرس'),\n",
       " ('صافيه', 'صافيه'),\n",
       " ('النيه', 'بودي'),\n",
       " ('احكي', 'الغوله'),\n",
       " ('قليلا', 'شوي'),\n",
       " ('وجده', 'لقاه'),\n",
       " ('استطيع', 'بقدر'),\n",
       " ('معرفه', 'العدو'),\n",
       " ('اخطات', 'غلطت'),\n",
       " ('انهيت', 'خلصت'),\n",
       " ('انترسبيتش', 'انترسبيتش'),\n",
       " ('كمال', 'كمال'),\n",
       " ('تزال', 'انترسبيتش'),\n",
       " ('ليون', 'ليون'),\n",
       " ('الماضي', 'زمان'),\n",
       " ('العب', 'العب'),\n",
       " ('احسن', 'احسن'),\n",
       " ('البارحه', 'مبارح'),\n",
       " ('الف', 'الف'),\n",
       " ('الاحسن', 'اضافيه'),\n",
       " ('اكتبه', 'اكتبو'),\n",
       " ('ارادت', 'تاخده'),\n",
       " ('اخذه', 'اخذه'),\n",
       " ('فلتاخذه', 'تاخده'),\n",
       " ('فريال', 'فريال'),\n",
       " ('الصباح', 'الصبح'),\n",
       " (':', 'بيتمسخروا'),\n",
       " ('رايته', 'شوفته'),\n",
       " ('الحلم', 'الحلم'),\n",
       " ('احبك', 'بحبك'),\n",
       " ('جدا', 'كتير'),\n",
       " ('التاسعه', 'التسعه'),\n",
       " ('عشر', 'اطنعش'),\n",
       " ('دقايق', 'دقايق'),\n",
       " ('العاشره', 'العشره'),\n",
       " ('باليد', 'باليد'),\n",
       " ('حيله', 'باليد'),\n",
       " ('الجامعه', 'الجامعه'),\n",
       " ('اعطتني', 'اعطتني'),\n",
       " ('العكس', 'العكس'),\n",
       " ('حب', 'حب'),\n",
       " ('تاتي', 'تجي'),\n",
       " ('قيمه', 'قيمه'),\n",
       " ('بالضبط', 'بالضبط'),\n",
       " ('نبين', 'بمجال'),\n",
       " ('مجال', 'مجال'),\n",
       " ('الاعلام', 'الاعلام'),\n",
       " ('الكثير', 'كتير'),\n",
       " ('الفرنسيه', 'الفرنسيه'),\n",
       " ('صدقيني', 'صدقيني'),\n",
       " ('تمزحين', 'شبكي'),\n",
       " ('يعلم', 'الحصانه'),\n",
       " ('خدمتك', 'الاغنيه'),\n",
       " ('رايعه', 'رايعه'),\n",
       " ('وصراحتا', 'الاغنيه'),\n",
       " ('اتعبتني', 'الاغنيه'),\n",
       " ('سهلا', 'سهل'),\n",
       " ('المراه', 'المراه'),\n",
       " ('تقول', 'تقول'),\n",
       " ('لزوجها', 'لجوزها'),\n",
       " ('وتزوج', 'وتجوز'),\n",
       " ('اخري', 'تانيه'),\n",
       " ('يطلب', 'تحاليل'),\n",
       " ('والاخر', 'والتاني'),\n",
       " ('يعطيك', 'يعطيك'),\n",
       " ('بالحفظ', 'بالحفظ'),\n",
       " ('كنا', 'كنا'),\n",
       " ('الشوارع', 'مشردين'),\n",
       " ('القريبه', 'القريبه'),\n",
       " ('منا', 'منا'),\n",
       " ('الحمد', 'الحمد'),\n",
       " ('لله', 'لله'),\n",
       " ('لمرورك', 'جعفريه'),\n",
       " ('الكريم', 'الكريم'),\n",
       " ('العزيزه', 'العزيزه'),\n",
       " ('جعفربه', 'جعفريه'),\n",
       " ('علويه', 'جعفريه'),\n",
       " ('ارحمنا', 'ارحمنا'),\n",
       " ('ياالله', 'ارحمنا'),\n",
       " ('بلاد', 'بلاد'),\n",
       " ('واسطه', 'واسطه'),\n",
       " ('انظري', 'شوفي'),\n",
       " ('حوار', 'حوار'),\n",
       " ('الحبيب', 'حوار'),\n",
       " ('حبيبته', 'حبيبته'),\n",
       " ('ارني', 'فرجيني'),\n",
       " ('يدك', 'ايدك'),\n",
       " ('امسكتك', 'مسكتك'),\n",
       " ('محرم', 'محرم'),\n",
       " ('قديم', 'قديم'),\n",
       " ('الرقم', 'الرقم'),\n",
       " ('انتظر', 'استني'),\n",
       " ('ساتصل', 'حاتصل'),\n",
       " ('الفتي', 'مسمي'),\n",
       " ('دكتور', 'يامهندسه'),\n",
       " ('سيد', 'سيد'),\n",
       " ('تعرفين', 'بتعرفي'),\n",
       " ('كدت', 'بروف'),\n",
       " ('واقول', 'الازهر'),\n",
       " ('فايسبوك', 'فيسبوك'),\n",
       " ('خاص', 'خاص'),\n",
       " ('بالاشخاص', 'اعراس'),\n",
       " ('كانت', 'كانت'),\n",
       " ('اريد', 'بدي'),\n",
       " ('انتظره', 'استناه'),\n",
       " ('اذهبي', 'روحي'),\n",
       " ('وفقك', 'قعده'),\n",
       " ('هزمنا', 'العدو'),\n",
       " ('ونعترف', 'العدو'),\n",
       " ('العدو', 'العدو'),\n",
       " ('يمكن', 'ممكن'),\n",
       " ('صديق', 'صديق'),\n",
       " ('ونعلم', 'العدو'),\n",
       " ('ابناينا', 'العدو'),\n",
       " ('عدونا', 'العدو'),\n",
       " ('راي', 'راي'),\n",
       " ('ابق', 'الوادي'),\n",
       " ('حالك', 'حالك'),\n",
       " ('انزلهم', 'الوادي'),\n",
       " ('جهه', 'جهه'),\n",
       " ('الوادي', 'الوادي'),\n",
       " ('نستفيد', 'نستفيد'),\n",
       " ('تخصصك', 'تخصصك'),\n",
       " ('نبيله', 'نبيله'),\n",
       " ('تعاني', 'ضغط'),\n",
       " ('اولادها', 'ولادها'),\n",
       " ('مكان', 'مكان'),\n",
       " ('الاخير', 'الاخير'),\n",
       " ('الصواب', 'اخرتها'),\n",
       " ('يفعله', 'اخرتها'),\n",
       " ('الفتيان', 'اخرتها'),\n",
       " ('طالب', 'طالب'),\n",
       " ('ماجيستير', 'اطار'),\n",
       " ('الفيزياء', 'اطار'),\n",
       " ('واريد', 'اطار'),\n",
       " ('اطار', 'اطار'),\n",
       " ('نظري', 'نظري'),\n",
       " ('ياعايشه', 'عيشه'),\n",
       " ('امان', 'امان'),\n",
       " ('احتمال', 'احتمال'),\n",
       " ('تبدو', 'مبينه'),\n",
       " ('اسمن', 'اسمن'),\n",
       " ('دايما', 'دايما'),\n",
       " ('تلبس', 'كعب'),\n",
       " ('عليا', 'بترد'),\n",
       " ('الفرق', 'الفرق'),\n",
       " ('يوجد', 'اخلاقهم'),\n",
       " ('نستطيع', 'بنقدر'),\n",
       " ('مدونه', 'مدونه'),\n",
       " ('احيانا', 'مرات'),\n",
       " ('تكلمني', 'عرفانه'),\n",
       " ('البدايه', 'البدايه'),\n",
       " ('شاء', 'شاء'),\n",
       " ('يا', 'يا'),\n",
       " ('ليتني', 'قرات'),\n",
       " ('قرات', 'قريت'),\n",
       " ('الرساله', 'الرساله'),\n",
       " ('فتره', 'ويكرمنا'),\n",
       " ('الدراسه', 'الدراسه'),\n",
       " ('امي', 'امي'),\n",
       " ('بطريقه', 'بطريقه'),\n",
       " ('خاصه', 'خاصه'),\n",
       " ('طلبوا', 'طلبوا'),\n",
       " ('ملف', 'ملف'),\n",
       " ('التسجيل', 'التسجيل'),\n",
       " ('اذكر', 'باتذكر'),\n",
       " ('اقامتها', 'الندوه'),\n",
       " ('عشره', 'بخمسطعشر'),\n",
       " ('ايام', 'ايام'),\n",
       " ('احب', 'بحب'),\n",
       " ('تضيفني', 'الاجره'),\n",
       " ('والشرف', 'الاجره'),\n",
       " ('الشتاء', 'الشتا'),\n",
       " ('الغسيل', 'الغسيل'),\n",
       " ('يلبس', 'بيلبس'),\n",
       " ('اولادي', 'ولادي'),\n",
       " ('قررت', 'قررت'),\n",
       " ('اره', 'شفتو'),\n",
       " ('اسام', 'لاطبخ'),\n",
       " ('السفر', 'السفر'),\n",
       " ('المحادثه', 'المحادثه'),\n",
       " ('انني', 'غلطانه'),\n",
       " ('الخواص', 'الخاص'),\n",
       " ('الانتظار', 'وبيحاول'),\n",
       " ('اثنين', 'تنين'),\n",
       " ('يحبوا', 'لاسباب'),\n",
       " ('يتزوجوا', 'بيحبو'),\n",
       " ('الطريقه', 'الطريقه'),\n",
       " ('العربيه', 'العربيه'),\n",
       " ('السلام', 'السلام'),\n",
       " ('بخير', 'بخير'),\n",
       " ('فريق', 'فريق'),\n",
       " ('يمكنني', 'ادرس'),\n",
       " ('ادرس', 'ادرس'),\n",
       " ('واعمل', 'واشتغل'),\n",
       " ('نلتقي', 'نتلاقي'),\n",
       " ('نتفق', 'نتفق'),\n",
       " ('طريقه', 'طريقه'),\n",
       " ('الكتابه', 'الكتابه'),\n",
       " ('جاء', 'اجا'),\n",
       " ('لخطبتها', 'زياره'),\n",
       " ('متزوجه', 'متجوزه'),\n",
       " ('بالذات', 'بالذات'),\n",
       " ('الحاله', 'الحاله'),\n",
       " ('القياده', 'القياده'),\n",
       " ('الحمل', 'القياده'),\n",
       " ('كبير', 'كبير'),\n",
       " ('اعتقد', 'اعتقد'),\n",
       " ('نعمل', 'نشتغل'),\n",
       " ('بشار', 'بشار'),\n",
       " ('المجنون', 'المجنون'),\n",
       " ('لاشيء', 'تليفون'),\n",
       " ('خيرا', 'خير'),\n",
       " ('حصلت', 'جابت'),\n",
       " ('رقم', 'رقم'),\n",
       " ('هاتف', 'تليفون'),\n",
       " ('الولايه', 'بالمحافظه'),\n",
       " ('سحبها', 'تسحبوها'),\n",
       " ('سيجعله', 'تسحبوها'),\n",
       " ('يتلاشي', 'تسحبوها'),\n",
       " ('وتتهدم', 'تسحبوها'),\n",
       " ('عيون', 'تسحبوها'),\n",
       " ('صهيون', 'تسحبوها'),\n",
       " ('وجاء', 'ريم'),\n",
       " ('البارح', 'البارح'),\n",
       " ('معناها', 'معناها'),\n",
       " ('بعيد', 'بعيد'),\n",
       " ('الغناء', 'الغناء'),\n",
       " ('خطا', 'غلط'),\n",
       " ('للامانه', 'بتسلم'),\n",
       " ('تكون', 'تكون'),\n",
       " ('فظاظه', 'بتسلم'),\n",
       " ('تلقي', 'بتسلم'),\n",
       " ('التحيه', 'الجديد'),\n",
       " ('الناس', 'الناس'),\n",
       " ('انهض', 'قوم'),\n",
       " ('صبيحه', 'صبيحه'),\n",
       " ('تسكن', 'ساكنه'),\n",
       " ('عزابه', 'عزابه'),\n",
       " ('تعلم', 'تعلم'),\n",
       " ('تقل', 'عالسوق'),\n",
       " ('شيا', 'اشي'),\n",
       " ('الثياب', 'الاواعي'),\n",
       " ('اقول', 'اقولك'),\n",
       " ('لسانك', 'لسانك'),\n",
       " ('خطي', 'خطي'),\n",
       " ('احاول', 'بحاول'),\n",
       " ('صدقني', 'صدقني'),\n",
       " ('الصديق', 'بيخجل'),\n",
       " ('يقف', 'بيوقف'),\n",
       " ('جانب', 'جنب'),\n",
       " ('صديقه', 'صديقه'),\n",
       " ('السراء', 'بيخجل'),\n",
       " ('الضراء', 'بيخجل'),\n",
       " ('يخجل', 'بيخجل'),\n",
       " ('يفرح', 'بيخجل'),\n",
       " ('لفرحه', 'بيخجل'),\n",
       " ('ويحزن', 'بيخجل'),\n",
       " ('لحزنه', 'بيخجل'),\n",
       " ('ويشاركك', 'بيخجل'),\n",
       " ('الراي', 'بيخجل'),\n",
       " ('الحكيم', 'بيخجل'),\n",
       " ('نصيره', 'نصيره'),\n",
       " ('اشياء', 'اشياء'),\n",
       " ('تفضفض', 'الغوله'),\n",
       " ('عنا', 'الغوله'),\n",
       " ('قصه', 'قصه'),\n",
       " ('اربعه', 'اربعه'),\n",
       " ('واربعون', 'الغوله'),\n",
       " ('ابو', 'ابو'),\n",
       " ('رجل', 'زلمه'),\n",
       " ('مسلوخه', 'الغوله'),\n",
       " ('امنا', 'الغوله'),\n",
       " ('الغوله', 'الغوله'),\n",
       " ('ادري', 'نشره'),\n",
       " ('اري', 'بشوف'),\n",
       " ('السماء', 'السماء'),\n",
       " ('المقبل', 'الجاي'),\n",
       " ('اعلم', 'بعرف'),\n",
       " ('وايضا', 'والكبار'),\n",
       " ('الكبار', 'والكبار'),\n",
       " ('مدركين', 'والكبار'),\n",
       " ('العلم', 'العلم'),\n",
       " ('متعلمون', 'والكبار'),\n",
       " ('احدي', 'ندوه'),\n",
       " ('المرات', 'المرات'),\n",
       " ('عرضا', 'عرض'),\n",
       " ('احضروا', 'جابوا'),\n",
       " ('الكلاب', 'الكلاب'),\n",
       " ('القسم', 'القسم'),\n",
       " ('ثمانيه', 'تمنيه'),\n",
       " ('التلاميذ', 'التلاميذ'),\n",
       " ('حال', 'حال'),\n",
       " ('والدتك', 'امك'),\n",
       " ('اطلب', 'اطلب'),\n",
       " ('كتابتي', 'كتابتي'),\n",
       " ('قريبه', 'قريبه'),\n",
       " ('مازالوا', 'محظوظين'),\n",
       " ('نرسل', 'اخوي'),\n",
       " ('جدتي', 'ستي'),\n",
       " ('اطول', 'اطول'),\n",
       " ('بثلاثه', 'برشلونه'),\n",
       " ('غريب', 'غريب'),\n",
       " ('واحيانا', 'ومرات'),\n",
       " ('الاسفل', 'والمدير'),\n",
       " ('اعرفها', 'بعرفها'),\n",
       " ('احسست', 'حسيت'),\n",
       " ('ربنا', 'يحققلنا'),\n",
       " ('يحقق', 'يحققلنا'),\n",
       " ('نحلم', 'يحققلنا'),\n",
       " ('ويساعدنا', 'يحققلنا'),\n",
       " ('اسعاد', 'يحققلنا'),\n",
       " ('اجهز', 'واختي'),\n",
       " ('حملت', 'حملت'),\n",
       " ('نقالا', 'حملت'),\n",
       " ('احصل', 'احصل'),\n",
       " ('لازالت', 'لساتها'),\n",
       " ('موجوده', 'موجوده'),\n",
       " ('اشارك', 'بشارك'),\n",
       " ('اشعر', 'حاسه'),\n",
       " ('شخص', 'شخص'),\n",
       " ('الدنيا', 'الدنيا'),\n",
       " ('قصير', 'قصير'),\n",
       " ('بالخصوص', 'زمن'),\n",
       " ('تخرج', 'الجوايز'),\n",
       " ('المدرسه', 'المدرسه'),\n",
       " ('ساعه', 'ساعه'),\n",
       " ('الزمن', 'الزمن'),\n",
       " ('تاخذ', 'بتتركهم'),\n",
       " ('قصيره', 'قصيره'),\n",
       " ('الجمعه', 'الجمعه'),\n",
       " ('الماضيه', 'الماضيه'),\n",
       " ('مرت', 'مرت'),\n",
       " ('الانسان', 'الانسان'),\n",
       " ('الصامت', 'الانسان'),\n",
       " ('يصلي', 'بيصلي'),\n",
       " ('مسجد', 'بلال'),\n",
       " ('بلال', 'بلال'),\n",
       " ('وابحثي', 'بالدارجه'),\n",
       " ('عمل', 'شغل'),\n",
       " ('اكملي', 'كملي'),\n",
       " ('عملك', 'شغلك'),\n",
       " ('يعني', 'يعني'),\n",
       " ('جمال', 'جمال'),\n",
       " ('تعطي', 'تمارين'),\n",
       " ('واحد', 'واحد'),\n",
       " ('مليون', 'مليون'),\n",
       " ('اسمع', 'اسمع'),\n",
       " ('يعطيه', 'بيعطيه'),\n",
       " ('ستقوم', 'عايده'),\n",
       " ('بدعوتها', 'اعزميها'),\n",
       " ('وزوجها', 'عزمتها'),\n",
       " ('يبطل', 'البحر'),\n",
       " ('السحر', 'السحر'),\n",
       " ('البحر', 'البحر'),\n",
       " ('بيتنا', 'بيتنا'),\n",
       " ('اسكت', 'اسكت'),\n",
       " ('دم', 'دم'),\n",
       " ('اختار', 'بختار'),\n",
       " ('انهي', 'خلص'),\n",
       " ('دراستي', 'دراستي'),\n",
       " ('ويمكن', 'ويمكن'),\n",
       " ('تكمل', 'تكمل'),\n",
       " ('المشروع', 'المشروع'),\n",
       " ('الخاص', 'نصيحتك'),\n",
       " ('جامعه', 'جامعه'),\n",
       " ('الطب', 'الطب'),\n",
       " ('وثلاثون', 'الاشتراك'),\n",
       " ('السابع', 'سبعه'),\n",
       " ('ديسمبر', 'كانون'),\n",
       " ('عنابه', 'عنابه'),\n",
       " ('اصبحت', 'صارت'),\n",
       " ('سيه', 'سيه'),\n",
       " ('غاليه', 'غاليه'),\n",
       " ('ميتان', 'سيه'),\n",
       " ('ثلاث', 'ثلاث'),\n",
       " ('ميه', 'باربعميه'),\n",
       " ('يامحمد', 'يامحمد'),\n",
       " ('الماستار', 'الماجستير'),\n",
       " ('الاول', 'الاول'),\n",
       " ('الثاني', 'الثاني'),\n",
       " ('ضعي', 'حطي'),\n",
       " ('الصوره', 'الصوره'),\n",
       " ('فهمت', 'فهمت'),\n",
       " ('يحبوك', 'حطي'),\n",
       " ('تفعلين', 'بتعملي'),\n",
       " ('نكتشف', 'البطيخه'),\n",
       " ('البطيخه', 'البطيخه'),\n",
       " ('فاسده', 'البطيخه'),\n",
       " ('امسكوا', 'غلطانه'),\n",
       " ('واجلسوني', 'غلطانه'),\n",
       " ('النهايه', 'النهايه'),\n",
       " ('قالوا', 'قالوا'),\n",
       " ('المخطا', 'غلطانه'),\n",
       " ('قولي', 'قوليلها'),\n",
       " ('سيسالونك', 'لهان'),\n",
       " ('ندفعهم', 'الاسلامي'),\n",
       " ('الاشياء', 'الاشياء'),\n",
       " ('والاستهزاء', 'الاسلامي'),\n",
       " ('بالدين', 'بالدين'),\n",
       " ('الاسلامي', 'الاسلامي'),\n",
       " ('نشاهد', 'الاسلامي'),\n",
       " ('القصبه', 'القصبه'),\n",
       " ('يعرفها', 'بيعرفها'),\n",
       " ('لاجل', 'بالالوان'),\n",
       " ('انوي', 'ناويه'),\n",
       " ('شعري', 'شعري'),\n",
       " ('كبيره', 'كبيره'),\n",
       " ('اتذكر', 'متذكره'),\n",
       " ('العطر', 'العطر'),\n",
       " ('السكايب', 'السكايب'),\n",
       " ('يجد', 'يلاقي'),\n",
       " ('احد', 'حدا'),\n",
       " ('علاج', 'علاج'),\n",
       " ('اعاني', 'علاج'),\n",
       " ('الازل', 'علاج'),\n",
       " ('سنوات', 'سنين'),\n",
       " ('الدور', 'الدور'),\n",
       " ('اتعلمين', 'بتعرفي'),\n",
       " ('ظهر', 'اصابعه'),\n",
       " ('والده', 'ابوه'),\n",
       " ('المخدرات', 'المخدرات'),\n",
       " ('النوم', 'النوم'),\n",
       " ('الدواء', 'الدوا'),\n",
       " ('استعملته', 'نوم'),\n",
       " ('قوي', 'قوي'),\n",
       " ('حذاء', 'جزمه'),\n",
       " ('كلمتني', 'سبع'),\n",
       " ('الاستاذ', 'الاستاذ'),\n",
       " ('بن', 'بن'),\n",
       " ('سبع', 'سبع'),\n",
       " ('الاجابه', 'الاجابه'),\n",
       " ('المعرفه', 'بتمني'),\n",
       " ('الفتاه', 'البنت'),\n",
       " ('رايتها', 'شوفتها'),\n",
       " ('يحب', 'بيحب'),\n",
       " ('يدرس', 'يدرس'),\n",
       " ('يومين', 'يومين'),\n",
       " ('تكونين', 'السكن'),\n",
       " ('رغم', 'التسعين'),\n",
       " ('وايل', 'وايل'),\n",
       " ('تخبر', 'تخبر'),\n",
       " ('اخواتها', 'لقينا'),\n",
       " ('البنات', 'البنات'),\n",
       " ('فضلك', 'سمحت'),\n",
       " ('ياكريمه', 'ياكريمه'),\n",
       " ('نقول', 'نقول'),\n",
       " ('تصرخي', 'حاج'),\n",
       " ('اخبارك', 'اخبارك'),\n",
       " ('تقوله', 'تقوله'),\n",
       " ('كلمه', 'كلمه'),\n",
       " ('سر', 'سر'),\n",
       " ('الذوق', 'الذوق'),\n",
       " ('طيبه', 'الذوق'),\n",
       " ('نخفف', 'الذوق'),\n",
       " ('انسان', 'انسان'),\n",
       " ('مجروح', 'الذوق'),\n",
       " ('تسعده', 'الذوق'),\n",
       " ('وكلمه', 'الذوق'),\n",
       " ('نتجنبها', 'الذوق'),\n",
       " ('توذي', 'الذوق'),\n",
       " ('مشاعره', 'الذوق'),\n",
       " ('ارايت', 'شوفتي'),\n",
       " ('ابنتك', 'بنتك'),\n",
       " ('الشكل', 'الشكل'),\n",
       " ('وزاد', 'وزاد'),\n",
       " ('الضغط', 'الضغط'),\n",
       " ('المصرين', 'وزاد'),\n",
       " ('تعريب', 'وزاد'),\n",
       " ('الدواوين', 'وزاد'),\n",
       " ('مسابقه', 'مسابقه'),\n",
       " ('رايك', 'رايك'),\n",
       " ('ساجد', 'حالاقي'),\n",
       " ('اصابعي', 'اصابعي'),\n",
       " ('ياحسام', 'ياحسام'),\n",
       " ('الخميس', 'الخميس'),\n",
       " ('فارغه', 'فاضيه'),\n",
       " ('طبيب', 'دكتور'),\n",
       " ('عام', 'عام'),\n",
       " ('تجري', 'تحاليل'),\n",
       " ('تحاليلا', 'تحاليل'),\n",
       " ('اعتبارا', 'اعتبار'),\n",
       " ('مبارك', 'مبارك'),\n",
       " ('فرعون', 'اعتبار'),\n",
       " ('فربما', 'اعتبار'),\n",
       " ('يرسل', 'اعتبار'),\n",
       " ('موسي', 'اعتبار'),\n",
       " ('مساكين', 'مساكين'),\n",
       " ('تكذبين', 'بتكذبي'),\n",
       " ('باعطايهم', 'تعطيهم'),\n",
       " ('الحلوي', 'سكره'),\n",
       " ('تذهبين', 'بتروحي'),\n",
       " ('بمفردك', 'لحالك'),\n",
       " ('عايشه', 'عايشه'),\n",
       " ('الخارج', 'برا'),\n",
       " ('اختبار', 'اختبار'),\n",
       " ('مستوي', 'مستوي'),\n",
       " ('صغار', 'صغار'),\n",
       " ('يسمعون', 'بيسمعوش'),\n",
       " ('التلفاز', 'التلفزيون'),\n",
       " ('يقولون', 'بيقولوا'),\n",
       " ('بيضاء', 'بيضاء'),\n",
       " ('فكرت', 'فكرت'),\n",
       " ('محظوظون', 'محظوظين'),\n",
       " ('يشرب', 'بيشرب'),\n",
       " ('اسال', 'اسال'),\n",
       " ('مهيدي', 'بنمهيدي'),\n",
       " ('رايع', 'روعه'),\n",
       " ('يبدا', 'يبدا'),\n",
       " ('عجله', 'مستعجله'),\n",
       " ('احتاج', 'الرد'),\n",
       " ('الاقل', 'الاقل'),\n",
       " ('اغادر', 'امشي'),\n",
       " ('العرب', 'العرب'),\n",
       " ('يعلمون', 'بيعرفوا'),\n",
       " ('الاعوام', 'السنين'),\n",
       " ('بنت', 'بنت'),\n",
       " ('نحيا', 'عايشين'),\n",
       " ('السلاح', 'السلاح'),\n",
       " ('يكن', 'كانش'),\n",
       " ('وقتا', 'بالدعوه'),\n",
       " ('ذاهبه', 'رايحه'),\n",
       " ('تشعر', 'بتحس'),\n",
       " ('اللازم', 'الاجابه'),\n",
       " ('خشيبات', 'عندهمش'),\n",
       " ('يابني', 'يابني'),\n",
       " ('استلف', 'يرجع'),\n",
       " ('الفا', 'الف'),\n",
       " ('ثانيه', 'البنر'),\n",
       " ('التلميذ', 'يرجع'),\n",
       " ('يسدد', 'يرجع'),\n",
       " ('تسلمي', 'الاستدعاء'),\n",
       " ('الاستدعاء', 'الاستدعاء'),\n",
       " ('بسرعه', 'بسرعه'),\n",
       " ('ياخذها', 'ياخذها'),\n",
       " ('فرق', 'فرق'),\n",
       " ('الساعات', 'النتيجه'),\n",
       " ('اذان', 'ادان'),\n",
       " ('العشاء', 'العشاء'),\n",
       " ('الصبح', 'اذان'),\n",
       " ('ثلاثه', 'ثلاثه'),\n",
       " ('وستحصلين', 'النتيجه'),\n",
       " ('النتيجه', 'النتيجه'),\n",
       " ('لاباس', 'اشلح'),\n",
       " ('الحاديه', 'الاحداعش'),\n",
       " ('ليلا', 'بالليل'),\n",
       " ('جالس', 'قاعد'),\n",
       " ('المطبخ', 'المطبخ'),\n",
       " ('اشاهد', 'بتفرج'),\n",
       " ('فيلما', 'فيلم'),\n",
       " ('المطعم', 'عالمطعم'),\n",
       " ('والاستاذ', 'والاستاذ'),\n",
       " ('ابراهيم', 'ابراهيم'),\n",
       " ('الدسوقي', 'والاستاذ'),\n",
       " ('كثير', 'والاستاذ'),\n",
       " ('الوقت', 'الوقت'),\n",
       " ('تملك', 'النجمه'),\n",
       " ('الاختيار', 'الاختيار'),\n",
       " ('الاحوال', 'الجويه'),\n",
       " ('دعي', 'للصبح'),\n",
       " ('ملاين', 'ملاين'),\n",
       " ('السيد', 'السيد'),\n",
       " ('دفع', 'اوراقه'),\n",
       " ('وذهب', 'اوراقه'),\n",
       " ('الامتحانات', 'الامتحانات'),\n",
       " ('الاستدراكيه', 'ايار'),\n",
       " ('قصتهم', 'قصتهم'),\n",
       " ('انتهت', 'قصتهم'),\n",
       " ('للمبيت', 'افريقيا'),\n",
       " ('احضرته', 'اعلان'),\n",
       " ('تري', 'تشوف'),\n",
       " ('خوله', 'خوله'),\n",
       " ('المسابقه', 'المسابقه'),\n",
       " ('سافعل', 'حاعمل'),\n",
       " ('بلدنا', 'بلدنا'),\n",
       " ('البرامج', 'المحجبه'),\n",
       " ('مكاني', 'تطوير'),\n",
       " ('ستفعلين', 'تطوير'),\n",
       " ('اللحظه', 'بهداك'),\n",
       " ('تحدثنا', 'حكينا'),\n",
       " ('السياسه', 'السياسه'),\n",
       " ('ياوداد', 'ياوداد'),\n",
       " ('تقولي', 'تقوليلي'),\n",
       " ('سياتي', 'حيجي'),\n",
       " ('ابوه', 'ابوه'),\n",
       " ('خالي', 'خالي'),\n",
       " ('اضرب', 'بضرب'),\n",
       " ('المدير', 'المدير'),\n",
       " ('تظهر', 'النص'),\n",
       " ('درس', 'درس'),\n",
       " ('محفظتين', 'العقل'),\n",
       " ('امه', 'امه'),\n",
       " ('عبدو', 'عبدو'),\n",
       " ('اخبرتني', 'السحر'),\n",
       " ('شينا', 'السحر'),\n",
       " ('يتم', 'انقطعت'),\n",
       " ('عمله', 'شغله'),\n",
       " ('للشخص', 'السحر'),\n",
       " ('يخطب', 'السحر'),\n",
       " ('تحميه', 'السحر'),\n",
       " ('الفن', 'بالفن'),\n",
       " ('يتعلق', 'ادوات'),\n",
       " ('القي', 'عايده'),\n",
       " ('ادوات', 'ادوات'),\n",
       " ('مدعوه', 'معزومه'),\n",
       " ('نكتب', 'نكتب'),\n",
       " ('المقالات', 'يافرعون'),\n",
       " ('اخبرنا', 'يافرعون'),\n",
       " ('الحل', 'الحل'),\n",
       " ('يافرعون', 'يافرعون'),\n",
       " ('بالتاكيد', 'بالتاكيد'),\n",
       " ('تنسخ', 'يافرعون'),\n",
       " ('المواضيع', 'يافرعون'),\n",
       " ('خاصتك', 'يافرعون'),\n",
       " ('افعل', 'اعمل'),\n",
       " ('تحب', 'بتحب'),\n",
       " ('والعياذ', 'والعياذ'),\n",
       " ('بالله', 'بالله'),\n",
       " ('ينسجم', 'ملامح'),\n",
       " ('ينزل', 'عيسي'),\n",
       " ('عيسي', 'عيسي'),\n",
       " ('ميتا', 'ميت'),\n",
       " ('نذهب', 'نروح'),\n",
       " ('رد', 'رد'),\n",
       " ('الزوج', 'بتتذكري'),\n",
       " ('تتذكري', 'بتتذكري'),\n",
       " ('تواعدنا', 'بتتذكري'),\n",
       " ('20', '20'),\n",
       " ('سنه', 'سنه'),\n",
       " ('بهاء', 'بهاء'),\n",
       " ('سلطان', 'سلطان'),\n",
       " ('قلبك', 'قلبك'),\n",
       " ('ياحول', 'بهاء'),\n",
       " ('شك', 'شك'),\n",
       " ('تعملين', 'بتشتغلي'),\n",
       " ('تسمع', 'تسمع'),\n",
       " ('والله', 'والله'),\n",
       " ('اختلطت', 'النظيفه'),\n",
       " ('مصر', 'مصر'),\n",
       " ('الحكومه', 'الحكومه'),\n",
       " ('الجديده', 'الجديده'),\n",
       " ('اكملت', 'كملت'),\n",
       " ('مكتبها', 'تعرفت'),\n",
       " ('تعرفت', 'تعرفت'),\n",
       " ('مشغول', 'مشغول'),\n",
       " ('تكلمت', 'التطبيع'),\n",
       " ('ابن', 'ابن'),\n",
       " ('عمي', 'عمي'),\n",
       " ('بخصوص', 'بخصوص'),\n",
       " ('الزواج', 'الجيزه'),\n",
       " ('تفهم', 'كتافك'),\n",
       " ('جاهز', 'كبرنا'),\n",
       " ('تفضلي', 'تفضلي'),\n",
       " ('الريس', 'الريس'),\n",
       " ('بوتفليقه', 'بوتفليقه'),\n",
       " ('اقصر', 'اقصر'),\n",
       " ('ريس', 'ريس'),\n",
       " ('تكره', 'غصب'),\n",
       " ('بالامر', 'غصب'),\n",
       " ('ارادي', 'غصب'),\n",
       " ('نجحت', 'نجحت'),\n",
       " ('خرج', 'طلع'),\n",
       " ('ابوك', 'ابوك'),\n",
       " ('باكرا', 'بدري'),\n",
       " ('ننتظر', 'طلعه'),\n",
       " ('لقلت', 'محلك'),\n",
       " ('اوجه', 'للاباء'),\n",
       " ('نخرج', 'ونحضر'),\n",
       " ('بيتها', 'بيتها'),\n",
       " ('واتذكر', 'وبتذكر'),\n",
       " ('النار', 'النار'),\n",
       " ('يريد', 'بدو'),\n",
       " ('جديدا', 'جديد'),\n",
       " ('تستطيع', 'مكانهم'),\n",
       " ('شهادات', 'شهادات'),\n",
       " ('النساء', 'النسوان'),\n",
       " ('تعرضن', 'للتحرش'),\n",
       " ('للتحرش', 'للتحرش'),\n",
       " ('وتضع', 'للتحرش'),\n",
       " ('والرعب', 'للتحرش'),\n",
       " ('والقلق', 'للتحرش'),\n",
       " ('والغثيان', 'للتحرش'),\n",
       " ('والكابوس', 'للتحرش'),\n",
       " ('يعيشن', 'للتحرش'),\n",
       " ('منزل', 'بيت'),\n",
       " ('رايحتها', 'ريحتها'),\n",
       " ('اعطاني', 'عطاني'),\n",
       " ('سيقول', 'وبسكر'),\n",
       " ('طالبه', 'طالبه'),\n",
       " ('افكر', 'الراس'),\n",
       " ('شعب', 'شعب'),\n",
       " ('وبشر', 'الراس'),\n",
       " ('فيمن', 'الراس'),\n",
       " ('الراس', 'الراس'),\n",
       " ('نداء', 'الراس'),\n",
       " ('تشاجر', 'مرتو'),\n",
       " ('احضر', 'جيب'),\n",
       " ('المحمول', 'اللابتوب'),\n",
       " ('تاتين', 'ابعتيلي'),\n",
       " ('دعنا', 'المحاكمه'),\n",
       " ('يااخي', 'يااخي'),\n",
       " ('نقف', 'قدامهم'),\n",
       " ('افضل', 'احسن'),\n",
       " ('يقفوا', 'قدامهم'),\n",
       " ('اغلي', 'اغلي'),\n",
       " ('الدره', 'اغلي'),\n",
       " ('بصراحه', 'بصراحه'),\n",
       " ('اكن', 'كنتش'),\n",
       " ('قادره', 'قادره'),\n",
       " ('انتظري', 'استني'),\n",
       " ('ساتكلم', 'لح'),\n",
       " ('ساره', 'ساره'),\n",
       " ('اعطيني', 'عطيني'),\n",
       " ('احضرتها', 'مكه'),\n",
       " ('وبدون', 'وبدون'),\n",
       " ('سابق', 'نتعرف'),\n",
       " ('انظار', 'نتعرف'),\n",
       " ('وبطال', 'نتعرف'),\n",
       " ('نتعرف', 'نتعرف'),\n",
       " ('ونصير', 'نتعرف'),\n",
       " ('اصدقاء', 'نتعرف'),\n",
       " ('فرض', 'فرض'),\n",
       " ('تعتادي', 'تتعودي'),\n",
       " ('فكره', 'فكره'),\n",
       " ('الكامرون', 'يلعب'),\n",
       " ('يريدون', 'الحصانه'),\n",
       " ('اللعب', 'اللعب'),\n",
       " ('يااختي', 'يااختي'),\n",
       " ('منال', 'منال'),\n",
       " ('طبعا', 'طبعا'),\n",
       " ('المفاجيه', 'كرامه'),\n",
       " ('تحدث', 'بتصير'),\n",
       " ('حديث', 'كرامه'),\n",
       " ('كرامه', 'كرامه'),\n",
       " ('تخيلوا', 'كرامه'),\n",
       " ('جماعه', 'جماعه'),\n",
       " ('ورده', 'ورده'),\n",
       " ('سيلتقون', 'جماعه'),\n",
       " ('مشكل', 'مشكله'),\n",
       " ('الاتصال', 'اتصل'),\n",
       " ('تتجاهلني', 'بتطنشني'),\n",
       " ('ايوب', 'ايوب'),\n",
       " ('واصل', 'بتطنشني'),\n",
       " ('اغلقت', 'سكرت'),\n",
       " ('اعدت', 'عدت'),\n",
       " ('فتحها', 'سكرت'),\n",
       " ('المفترض', 'التسعين'),\n",
       " ('الشقه', 'واختي'),\n",
       " ('باكملها', 'واختي'),\n",
       " ('واختي', 'واختي'),\n",
       " ('حضرت', 'واشترت'),\n",
       " ('الثامنه', 'التمنيه'),\n",
       " ('بالاضافه', 'بالاضافه'),\n",
       " ('ممكن', 'اعشاب'),\n",
       " ('منقاه', 'اعشاب'),\n",
       " ('تحتوي', 'اعشاب'),\n",
       " ('اعشاب', 'اعشاب'),\n",
       " ('ضاره', 'اعشاب'),\n",
       " ('سامه', 'اعشاب'),\n",
       " ('الشيخ', 'طيران'),\n",
       " ('بشده', 'احق'),\n",
       " ('احبهم', 'اعراس'),\n",
       " ('القول', 'زواج'),\n",
       " ('عاودت', 'رديت'),\n",
       " ('اجبت', 'رديت'),\n",
       " ('اسالكم', 'احكولي'),\n",
       " ('المدونه', 'المدونه'),\n",
       " ('متنوعه', 'متنوعه'),\n",
       " ('العطله', 'العطله'),\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": بيتمسخروا\n",
      "20 20\n",
      "« na3saa\n",
      "» na3saa\n",
      "362 362\n",
      "للامثال 362\n",
      "العديد na3saa\n",
      "12 12\n",
      "الحادي 11\n",
      "na3saa na3saa\n",
      "@ na3saa\n",
      "hotmailcom na3saa\n",
      "… اهل\n",
      "اللاختيار 11\n",
      "الوحده 1\n",
      "تتخيلوا 700\n",
      "700 700\n",
      "ريال 700\n",
      "الحيله 700\n"
     ]
    }
   ],
   "source": [
    "msa_wv_list=[]\n",
    "lav_wv_list=[]\n",
    "for i,j in word_pairs:\n",
    "    if i in m_msa.wv.key_to_index and j in m_lav.wv.key_to_index :\n",
    "        msa_wv_list.append(m_msa.wv.key_to_index[i])\n",
    "        lav_wv_list.append(m_lav.wv.key_to_index[j])\n",
    "    else:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6099"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msa_wv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('اعرف', 'بعرف')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_msa.wv.index_to_key[27],m_lav.wv.index_to_key[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_wv_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_msa_emb,val_msa_emb = m_msa.wv.vectors[msa_wv_list],m_msa.wv.vectors[msa_wv_list][5500:]\n",
    "trn_lav_emb,val_lav_emb = m_lav.wv.vectors[lav_wv_list],m_lav.wv.vectors[lav_wv_list][5500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = gensim.models.Word2Vec.load('../models/full_grams_cbow_100_wiki/full_grams_cbow_100_wiki.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_wv_list=[]\n",
    "lav_wv_list=[]\n",
    "for i,j in word_pairs:\n",
    "    if i in t_model.wv.key_to_index and j in t_model.wv.key_to_index :\n",
    "        msa_wv_list.append(t_model.wv.key_to_index[i])\n",
    "        lav_wv_list.append(t_model.wv.key_to_index[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lav_wv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('اعرف', 'بعرف')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.wv.index_to_key[20183],t_model.wv.index_to_key[100521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7103628636809415"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c/len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_msa_emb = t_model.wv.vectors[msa_wv_list]\n",
    "trn_lav_emb = t_model.wv.vectors[lav_wv_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Frobenius square loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X,Y,R):\n",
    "    diff_squared = (torch.dot(X,R) - Y)**2\n",
    "    loss = torch.sum(diff_squared) / X.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    return np.dot(X.T,(np.dot(X,R)-Y)) * (2/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "    #    if i % 25 == 0:\n",
    "    #        print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X,Y,R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "        ### END CODE HERE ###\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# correct examples for lr: 0.1 and steps of 400 = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amr/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# correct examples for lr: 0.1 and steps of 600 = 1\n",
      "# correct examples for lr: 0.1 and steps of 800 = 1\n",
      "# correct examples for lr: 0.1 and steps of 1000 = 1\n",
      "# correct examples for lr: 0.1 and steps of 1200 = 1\n",
      "# correct examples for lr: 0.2 and steps of 400 = 1\n",
      "# correct examples for lr: 0.2 and steps of 600 = 1\n",
      "# correct examples for lr: 0.2 and steps of 800 = 1\n",
      "# correct examples for lr: 0.2 and steps of 1000 = 1\n",
      "# correct examples for lr: 0.2 and steps of 1200 = 1\n",
      "# correct examples for lr: 0.4 and steps of 400 = 1\n",
      "# correct examples for lr: 0.4 and steps of 600 = 1\n",
      "# correct examples for lr: 0.4 and steps of 800 = 1\n",
      "# correct examples for lr: 0.4 and steps of 1000 = 1\n",
      "# correct examples for lr: 0.4 and steps of 1200 = 1\n",
      "# correct examples for lr: 0.6 and steps of 400 = 1\n",
      "# correct examples for lr: 0.6 and steps of 600 = 1\n",
      "# correct examples for lr: 0.6 and steps of 800 = 1\n",
      "# correct examples for lr: 0.6 and steps of 1000 = 1\n",
      "# correct examples for lr: 0.6 and steps of 1200 = 1\n",
      "# correct examples for lr: 0.8 and steps of 400 = 1\n",
      "# correct examples for lr: 0.8 and steps of 600 = 1\n",
      "# correct examples for lr: 0.8 and steps of 800 = 1\n",
      "# correct examples for lr: 0.8 and steps of 1000 = 1\n",
      "# correct examples for lr: 0.8 and steps of 1200 = 1\n",
      "# correct examples for lr: 1 and steps of 400 = 1\n",
      "# correct examples for lr: 1 and steps of 600 = 1\n",
      "# correct examples for lr: 1 and steps of 800 = 1\n",
      "# correct examples for lr: 1 and steps of 1000 = 1\n",
      "# correct examples for lr: 1 and steps of 1200 = 1\n",
      "# correct examples for lr: 1.5 and steps of 400 = 1\n",
      "# correct examples for lr: 1.5 and steps of 600 = 1\n",
      "# correct examples for lr: 1.5 and steps of 800 = 1\n",
      "# correct examples for lr: 1.5 and steps of 1000 = 1\n",
      "# correct examples for lr: 1.5 and steps of 1200 = 1\n",
      "# correct examples for lr: 2 and steps of 400 = 1\n",
      "# correct examples for lr: 2 and steps of 600 = 1\n",
      "# correct examples for lr: 2 and steps of 800 = 1\n",
      "# correct examples for lr: 2 and steps of 1000 = 1\n",
      "# correct examples for lr: 2 and steps of 1200 = 1\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1,0.2,0.4,0.6,0.8,1,1.5,2]:\n",
    "    for steps in [400,600,800,1000,1200]:\n",
    "        R_train = align_embeddings(trn_msa_emb, trn_lav_emb, train_steps=steps, learning_rate=lr)\n",
    "        #Testing\n",
    "        X = np.dot(trn_msa_emb,R_train)\n",
    "        sim_matrix = vectorized_cos_sim_cuda(X,trn_lav_emb)\n",
    "        ret = np.argmax(sim_matrix,axis=1)\n",
    "        c = (ret == range(len(trn_lav_emb))).sum()\n",
    "        print(f'# correct examples for lr: {lr} and steps of {steps} = {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.dot(trn_msa_emb,R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6099, 300)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = vectorized_cos_sim_cuda(X,trn_lav_emb)\n",
    "ret = np.argmax(sim_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ret == range(len(trn_lav_emb))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arrDs(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.data=torch.from_numpy(X)\n",
    "        self.labels=torch.from_numpy(Y)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.stack([self.data[idx],self.labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frob(nn.Module):    \n",
    "    def __call__(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "        # diff is XR - Y\n",
    "        diff = X - Y\n",
    "        # diff_squared is the element-wise square of the difference\n",
    "        diff_squared = diff**2\n",
    "        # sum_diff_squared is the sum of the squared elements\n",
    "        sum_diff_squared = torch.sum(diff_squared)\n",
    "        # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "        loss = sum_diff_squared / m\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "frob_loss=frob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Sequential(nn.Linear(100,100,bias=True),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(100,100,bias=True),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(100,100,bias=True))\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sced = torch.optim.lr_scheduler.StepLR(optim,200,gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1418, grad_fn=<MseLossBackward>)\n",
      "0\n",
      "tensor(2.9455, grad_fn=<MseLossBackward>)\n",
      "4\n",
      "tensor(2.8687, grad_fn=<MseLossBackward>)\n",
      "8\n",
      "tensor(2.7941, grad_fn=<MseLossBackward>)\n",
      "27\n",
      "tensor(2.7096, grad_fn=<MseLossBackward>)\n",
      "101\n",
      "tensor(2.6280, grad_fn=<MseLossBackward>)\n",
      "234\n",
      "tensor(2.5588, grad_fn=<MseLossBackward>)\n",
      "386\n",
      "tensor(2.5015, grad_fn=<MseLossBackward>)\n",
      "556\n",
      "tensor(2.4529, grad_fn=<MseLossBackward>)\n",
      "689\n",
      "tensor(2.4107, grad_fn=<MseLossBackward>)\n",
      "821\n",
      "tensor(2.3737, grad_fn=<MseLossBackward>)\n",
      "933\n",
      "tensor(2.3412, grad_fn=<MseLossBackward>)\n",
      "1014\n",
      "tensor(2.3126, grad_fn=<MseLossBackward>)\n",
      "1064\n",
      "tensor(2.2870, grad_fn=<MseLossBackward>)\n",
      "1117\n",
      "tensor(2.2641, grad_fn=<MseLossBackward>)\n",
      "1158\n",
      "tensor(2.2435, grad_fn=<MseLossBackward>)\n",
      "1186\n",
      "tensor(2.2248, grad_fn=<MseLossBackward>)\n",
      "1214\n",
      "tensor(2.2076, grad_fn=<MseLossBackward>)\n",
      "1235\n",
      "tensor(2.1917, grad_fn=<MseLossBackward>)\n",
      "1256\n",
      "tensor(2.1776, grad_fn=<MseLossBackward>)\n",
      "1265\n",
      "tensor(2.2024, grad_fn=<MseLossBackward>)\n",
      "1273\n",
      "tensor(2.1523, grad_fn=<MseLossBackward>)\n",
      "1272\n",
      "tensor(2.1526, grad_fn=<MseLossBackward>)\n",
      "1278\n",
      "tensor(2.1332, grad_fn=<MseLossBackward>)\n",
      "1287\n",
      "tensor(2.1274, grad_fn=<MseLossBackward>)\n",
      "1294\n",
      "tensor(2.1177, grad_fn=<MseLossBackward>)\n",
      "1299\n",
      "tensor(2.1123, grad_fn=<MseLossBackward>)\n",
      "1298\n",
      "tensor(2.1010, grad_fn=<MseLossBackward>)\n",
      "1287\n",
      "tensor(2.1124, grad_fn=<MseLossBackward>)\n",
      "1325\n",
      "tensor(2.0813, grad_fn=<MseLossBackward>)\n",
      "1301\n",
      "tensor(2.0873, grad_fn=<MseLossBackward>)\n",
      "1310\n",
      "tensor(2.0684, grad_fn=<MseLossBackward>)\n",
      "1313\n"
     ]
    }
   ],
   "source": [
    "for i in range(800):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    data = torch.from_numpy(trn_msa_emb)\n",
    "    label = torch.from_numpy(trn_lav_emb)\n",
    "    op = model(data)\n",
    "    loss = mse_loss(op,label)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i%25==0:\n",
    "        print(loss)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            X = model(torch.from_numpy(trn_msa_emb))\n",
    "            sim_matrix = vectorized_cos_sim_cuda(X.detach().numpy(),trn_lav_emb)\n",
    "            ret = np.argmax(sim_matrix,axis=1)\n",
    "            print((ret == range(len(trn_lav_emb))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model(torch.from_numpy(trn_msa_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = vectorized_cos_sim_cuda(X.detach().numpy(),trn_lav_emb)\n",
    "ret = np.argmax(sim_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ret == range(len(trn_lav_emb))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9592.1963, grad_fn=<DivBackward0>)\n",
      "tensor(5323.6069, grad_fn=<DivBackward0>)\n",
      "tensor(5444.4238, grad_fn=<DivBackward0>)\n",
      "tensor(4764.9780, grad_fn=<DivBackward0>)\n",
      "tensor(5485.4663, grad_fn=<DivBackward0>)\n",
      "tensor(6249.7515, grad_fn=<DivBackward0>)\n",
      "tensor(4756.2710, grad_fn=<DivBackward0>)\n",
      "tensor(5128.9648, grad_fn=<DivBackward0>)\n",
      "tensor(5056.0591, grad_fn=<DivBackward0>)\n",
      "tensor(5009.1401, grad_fn=<DivBackward0>)\n",
      "tensor(5340.9111, grad_fn=<DivBackward0>)\n",
      "tensor(5592.8408, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.from_numpy(trn_msa_emb)\n",
    "label = torch.from_numpy(trn_lav_emb)\n",
    "w1 = torch.rand(100,100,requires_grad=True)\n",
    "lr= 1e-2\n",
    "for i in range(600):\n",
    "    op = torch.matmul(data,w1)\n",
    "    loss = frob_loss(op,label)\n",
    "    loss.backward()\n",
    "    if i%50==0:\n",
    "        print(loss)\n",
    "    with torch.no_grad():\n",
    "        w1 -=  lr*w1.grad\n",
    "    w1.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.matmul(data,w1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = vectorized_cos_sim_cuda(X.detach().numpy(),label.detach().numpy())\n",
    "ret = np.argmax(sim_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ret == range(len(trn_lav_emb))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1029, -7.9874, -6.5976,  ...,  1.9660, -5.4957, -4.1504],\n",
       "        [13.0289, 10.8297,  4.4311,  ...,  5.7603,  0.1946, -4.6342],\n",
       "        [-3.4615,  1.5145,  0.0152,  ..., -9.1015, -3.6867, -6.0899],\n",
       "        ...,\n",
       "        [-0.5324, -0.2738, -0.4118,  ..., -0.0648, -0.9023, -0.4321],\n",
       "        [-0.7898, -1.1300, -0.3995,  ..., -0.7782, -0.9758, -0.0220],\n",
       "        [-0.5042, -1.3221,  4.0849,  ..., -5.8279, -2.3902, -7.8763]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "\n",
    "    # for each candidate vector...  \n",
    "    similarity_l = vectorized_cos_sim(v,candidates)\n",
    "  \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l).squeeze()\n",
    "\n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    ### END CODE HERE ###\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i],Y,k=1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct/X.shape[0]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-0ac819ebe1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_msa_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrn_lav_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-7187ef599dff>\u001b[0m in \u001b[0;36mtest_vocabulary\u001b[0;34m(X, Y, R)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpred_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_neighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# if the index of the nearest neighbor equals the row of i... \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-038a32e653ae>\u001b[0m in \u001b[0;36mnearest_neighbor\u001b[0;34m(v, candidates, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for each candidate vector...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msimilarity_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_cos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# sort the similarity list and get the indices of the sorted list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-5a55c24f06f8>\u001b[0m in \u001b[0;36mvectorized_cos_sim\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvectorized_cos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmod_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmod_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2559\u001b[0m             \u001b[0;31m# special case for speedup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m         \u001b[0;31m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# are valid for vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(trn_msa_emb,trn_lav_emb, R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
